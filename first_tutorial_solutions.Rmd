---
title: "first tutorial solutions"
author: "Niklas Cypris"
date: "19 2 2021"
output: pdf_document
---

```{r setup}
library(tidyverse)
library(paramtest)
library(parallel)
```

# SIMULATION FUNCTION

y = b0 + b1 * x + error

```{r}
set.seed(161)
lm_sim <- function(simNum, N, b1, b0 = 0) #necessary parameters
{
  
  con <- rnorm(N, 0, 1)
  
  # con %>% as_tibble() %>%
  # ggplot(aes(x = round(value, 1))) +
  #   geom_bar() +
  #   xlab("conscientiousness") +
  #   theme_classic()
  
  rerror <- rnorm(N, 0, sqrt(1 - b1^2))

  # rerror %>% as_tibble() %>%
  # ggplot(aes(x = round(value, 1))) +
  #   geom_bar() +
  #   xlab("pro_env") +
  #   theme_classic()
    
  y = b1*con + rerror
  
  model1 = lm(y ~ con)
  
  # model 1
  est1_x1 <- coef(summary(model1))['con', 'Estimate']
  p1_x1 <- coef(summary(model1))['con', 'Pr(>|t|)']
  sig1_x1 <- p1_x1 < .05
  
  return(c(est1_x1 = est1_x1, p1_x1 = p1_x1, sig1_x1 = sig1_x1))
  
}
```

## PARALLEL ITERATIONS

Detect number of cores that your computer can use for performing parallel iterations.

```{r}
detectCores()
```

# SIMULATE DATA

```{r}
power_lm <- grid_search(lm_sim, params=list(N = c(150, 200, 250, 300)),
                         n.iter = 1000, output ='data.frame', b1 = .2, parallel = 'snow', ncpus = 4)
```

# DISPLAY RESULTS

```{r}
power_lm %>%
  results() %>% 
  group_by(N.test) %>%
  summarise(
    beta_con = mean(est1_x1),
    power = mean(sig1_x1)
  )
```

```{r}
power_lm %>% 
  results() %>% 
  group_by(N.test) %>% 
  summarise(
    beta_con = mean(est1_x1),
    power = mean(sig1_x1)
  ) %>% 
  ggplot(aes(x = N.test, y = power)) +
  geom_line() +
  theme_classic()
```

```{r}
power_lm %>% 
  results() %>% 
  ggplot(aes(x = round(p1_x1, 2))) +
  geom_bar() +
  geom_vline(xintercept = .05, color = "red") +
  facet_wrap(~ N.test, nrow = 1) +
  xlab("p values") +
  xlim(-0.01, 0.5) +
  theme_classic()
```



# EXERCISES

## 1. Vary the effect size of the b1-coefficient (.1, .2, .3) and the number of participants (50, 100, 150).

```{r}
power_lm1 <- grid_search(lm_sim, params=list(N = c(50, 100, 150), b1 = c(.1, .2, .3)),
                        n.iter = 1000, output ='data.frame', parallel = 'snow', ncpus = 4)
```

```{r}
power_lm1 %>%
  results() %>% 
  group_by(N.test, b1.test) %>%
  summarise(
    beta_con = mean(est1_x1),
    power = mean(sig1_x1)
  )
```

```{r}
power_lm1 %>% 
  results() %>% 
  mutate(b1_size = as.factor(b1.test)) %>% 
  group_by(N.test, b1_size) %>% 
  summarise(
    beta_con = mean(est1_x1),
    power = mean(sig1_x1)
  ) %>% 
  ggplot(aes(x = N.test, y = power, color = b1_size)) +
  geom_line() +
  theme_classic()
```

```{r}
power_lm1 %>% 
  results() %>% 
  ggplot(aes(x = round(p1_x1, 2))) +
  geom_bar() +
  geom_vline(xintercept = .05, color = "red") +
  facet_wrap(vars(N.test, b1.test)) +
  xlab("p values") +
  xlim(-0.01, 0.5) +
  theme_classic()
```

## 2. Add another predictor to the equation. Adjust output (e.g. est1_x2 etc) and results table

```{r}
lm_sim2 <- function(simNum, N, b1, b2, bx, b0=0)
{
  
  con <- rnorm(N, 0, 1)
  pol <- rnorm(N, 0, 1) # adding political orientation as a predictor
  rerror <- rnorm(N, 0, sqrt(1 - b1^2 - b2^2 - bx^2))
  y = b1*con + b2*pol + bx * con * pol + rerror
  model1 = lm(y ~ con * pol)

  # get conscientiousness
  est1_x1 <- coef(summary(model1))['con', 'Estimate']
  p1_x1 <- coef(summary(model1))['con', 'Pr(>|t|)']
  sig1_x1 <- p1_x1 < .05
  
  # get political orientation
  est1_x2 <- coef(summary(model1))['pol', 'Estimate']
  p1_x2 <- coef(summary(model1))['pol', 'Pr(>|t|)']
  sig1_x2 <- p1_x2 < .05
  
  # get interaction
  est1_xx <- coef(summary(model1))['con:pol', 'Estimate']
  p1_xx <- coef(summary(model1))['con:pol', 'Pr(>|t|)']
  sig1_xx <- p1_xx < .05
  
  return(c(est1_x1 = est1_x1, p1_x1 = p1_x1, sig1_x1 = sig1_x1,
           est1_x2 = est1_x2, p1_x2 = p1_x2, sig1_x2 = sig1_x2,
           est1_xx = est1_xx, p1_xx = p1_xx, sig1_xx = sig1_xx))
  
}
```

```{r}
power_lm2 <- grid_search(lm_sim2, params=list(N = c(150, 200, 250, 300)),
                        n.iter = 1000, output ='data.frame', b1 = .2, b2 = .1, bx = .2, parallel = 'snow', ncpus = 4)
```

```{r}
power_lm2 %>%
  results() %>% 
  group_by(N.test) %>%
  summarise(
    beta_con = mean(est1_x1),
    power_con = mean(sig1_x1),
    beta_pol = mean(est1_x2),
    power_pol = mean(sig1_x2),
    beta_x = mean(est1_xx),
    power_x = mean(sig1_xx)
  )
```

```{r}
power_lm2 %>% 
  results() %>% 
  group_by(N.test) %>% 
  summarise(
    power_con = mean(sig1_x1),
    power_pol = mean(sig1_x2),
    power_x = mean(sig1_xx)) %>% 
  pivot_longer(cols = starts_with("power"), names_to = "predictors", values_to = "power") %>% 
  ggplot(aes(x = N.test, y = power, color = predictors)) +
  geom_line() +
  theme_classic()
```

p-values become too cluttered at this level.

## 3. Compare model1 with one predictor (b1 = .05) and model2 with two predictors (b1 = .05, b2 = .2, bx = .05).

```{r}
comp_sim <- function(simNum, N, b1, b2, bx, b0=0)
{
  
  con <- rnorm(N, 0, 1)
  pol <- rnorm(N, 0, 1)
  rerror <- sqrt(1 - b1^2 - b2^2)
  y = b1*con + b2*pol + bx * con * pol + rnorm(N, 0, rerror)
  model1 = lm(y ~ con)
  model2 = lm(y ~ con * pol)
  
  # model comparison
  comp_p <- anova(model1, model2)[2, "Pr(>F)"]
  comp_sig <- comp_p < .05
  
  return(c(comp_p = comp_p, comp_sig = comp_sig))
  
}
```

```{r}
power_comp <- grid_search(comp_sim, params=list(N = c(150, 200, 250, 300)),
                         n.iter = 1000, output ='data.frame', b1 = .05, b2 = .2, bx = .05, parallel = 'snow', ncpus = 4)
```

```{r}
power_comp %>%
  results() %>% 
  group_by(N.test) %>%
  summarise(
    power = mean(comp_sig))
```

```{r}
power_comp %>% 
  results() %>% 
  group_by(N.test) %>% 
  summarise(
    power = mean(comp_sig)
  ) %>% 
  ggplot(aes(x = N.test, y = power)) +
  geom_line() +
  theme_classic()
```

```{r}
power_comp %>% 
  results() %>% 
  ggplot(aes(x = round(comp_p, 2))) +
  geom_bar() +
  geom_vline(xintercept = .05, color = "red") +
  facet_wrap(~ N.test, nrow = 1) +
  xlab("p values") +
  xlim(-0.01, 0.5) +
  theme_classic()
```

## 4. Simulate Data for a t-test.

```{r}
t_sim <- function(simNum, N, d) {
  x1 <- rnorm(N, 0, 1)
  x2 <- rnorm(N, d, 1)
  
  t <- t.test(x1, x2, var.equal=FALSE)
  t_stat <- t$statistic
  p <- t$p.value
  sig <- p < .05
  
  return(c(t_stat = t_stat, p = p, sig = sig))
  
}
```

```{r}
power_t <- grid_search(t_sim, params=list(N = c(150, 200, 250, 300)), n.iter = 1000, output='data.frame', d = .3)
```

```{r}
power_t %>%
  results() %>% 
  group_by(N.test) %>%
  summarise(power = mean(sig))
```

```{r}
power_t %>% 
  results() %>% 
  group_by(N.test) %>% 
  summarise(
    power = mean(sig)
  ) %>% 
  ggplot(aes(x = N.test, y = power)) +
  geom_line() +
  theme_classic()
```

```{r}
power_t %>% 
  results() %>% 
  ggplot(aes(x = round(p, 2))) +
  geom_bar() +
  geom_vline(xintercept = .05, color = "red") +
  facet_wrap(~ N.test, nrow = 1) +
  xlab("p values") +
  xlim(-0.01, 0.5) +
  theme_classic()
```
